{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1-The Reinforcement Learning Problem\n",
    "- 从交互（interaction）中学习是几乎所有学习和智能理论的基石（underlying idea）。\n",
    "- 相比于其他机器学习方法，强化学习更加集中在goal-directed learning上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reinforcement Learning\n",
    "- 强化学习区别与其他机器学习方法的三个特征\n",
    "    - 闭环：学习系统的动作会影响其后续的输入\n",
    "    - 对采取何种动作并不做直接的指示\n",
    "    - 动作的结果，包括奖励型号，将在更广的时间范围内有影响。\n",
    "\n",
    "- MDP formulation is intended to include just these three aspects:\n",
    "    - sensation: 感知环境状态\n",
    "    - action: 采取行动影响状态\n",
    "    - goal: agent必须具有和环境状态有关的目标\n",
    "\n",
    "- 强化学习的一个挑战是在探索（exploration）和利用（exploitation）之间权衡。\n",
    "    - The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make better action selections in the future。\n",
    "\n",
    "- Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. 另一个强化学习的关键特征是为其显式的考虑了与未知环境交互的目标导向的agent的整个问题，而非一些子问题。\n",
    "\n",
    "- goal-seeking agent可以指示更大系统的一个组成部分，其直接与大系统的剩余部分交互，间接地与大系统所处的环境交互。\n",
    "\n",
    "- 强化学习是人工智能回归simple general principle这一潮流的一部分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Element of Reinforcement Learning\n",
    "强化学习系统的主要组成单元：\n",
    "- Policy：定义了一个learning agent在给定时间的动作。粗略来讲，policy是从感知到的环境状态到该状态下采取的动作的映射。\n",
    "- Reward signal：定义了强化学习问题的目标。在每一个时间步，环境传递给强化学习agent一个数值，即一个reward。\n",
    "    - reward signal indicates what is good in an immediate sense.\n",
    "    - agent的唯一的目标即为最大化其长期接收到的总reward。\n",
    "    - 生成奖励信号的过程不能被agent改变，即产生信号的函数不能被改变。\n",
    "- Value function：粗略的说，状态的价值是从该状态开始，未来一个agent可以接收到的总reward的期望。A value specifies what is good in the long run.\n",
    "    - 事实上，几乎素有强化学习算法中最重要的组成部分是高效估计value的方法。\n",
    "- model of environment：This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limitations and Scope\n",
    "- evolutionary methods：诸如遗传算法、遗传规划、模拟退火等优化算法。这些方法评估许多non-learning agent“一生”的表现，每个agent使用不同的policy和环境交互，最终选择能够获得做多（长期）reward的agent（及他们对应的policy）。\n",
    "    - 如果策略空间足够小，或者可以被结构化从而好的策略是常见的且易于被发现的，或者允许花费大量的时间去搜索，那么金华方法是有效的。此外，进化方法在learning agent不能精确感知环境状态的问题上更具优势。\n",
    "\n",
    "- 本书将集中在涉及环境交互的强化学习方法上，而非evolutionary method。\n",
    "- policy gradient methods\n",
    "- unpredictability of optimization：可能会有些意想不到的后果，应用时需小心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 An Extended Example: Tic-Tac-Toe\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
